{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive environ: Jupyter themes not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/trimtf/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import architectures as arch\n",
    "from functools import partial\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from torchvision.ops import MLP\n",
    "from data import load_navier_stokes_tensor, setup_dataloaders\n",
    "from training import build_pipeline\n",
    "from training import Trainer\n",
    "from architectures import SingleConvNeuralNet\n",
    "from trim_transformer.transformer_layers import TrimTransformerEncoderLayer, TrimTransformerEncoder\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Navierâ€“Stokes training script.\")\n",
    "parser.add_argument(\"--name\", type=str, default=\"test\")\n",
    "parser.add_argument(\"--data\", type=Path, default=Path(\"../ns_data.mat\"), help=\"Path to the .mat dataset.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "parser.add_argument(\"--batch-size\", type=int, default=8)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "parser.add_argument(\"--weight-decay\", type=float, default=1e-4, help=\"Weight decay (L2 penalty) for Adam optimizer.\")\n",
    "parser.add_argument(\"--n-timesteps\", type=int, default=11, help=\"Number of temporal frames to sample from the raw data (consistent with notebook).\")\n",
    "\n",
    "parser.add_argument(\"--share\", action=\"store_true\", help=\"Share weights between modules.\")\n",
    "parser.add_argument(\"--no-share\", dest=\"share\", action=\"store_false\", help=\"Don't share weights between modules.\")\n",
    "parser.set_defaults(share=True)\n",
    "\n",
    "parser.add_argument(\"--refinement\", action=\"store_true\", help=\"Use refinement.\")\n",
    "parser.add_argument(\"--no-refinement\", dest=\"refinement\", action=\"store_false\", help=\"Don't use refinement.\")\n",
    "parser.set_defaults(refinement=True)\n",
    "\n",
    "parser.add_argument(\"--picard\", action=\"store_true\", help=\"Use Picard iterations.\")\n",
    "parser.add_argument(\"--no-picard\", dest=\"picard\", action=\"store_false\", help=\"Don't use Picard iterations.\")\n",
    "parser.set_defaults(picard=True)\n",
    "\n",
    "parser.add_argument(\"--d_model\", type=int, default=64)\n",
    "parser.add_argument(\"--nhead\", type=int, default=4)\n",
    "parser.add_argument(\"--dim_feedforward\", type=int, default=64)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "parser.add_argument(\"--n_layers\", type=int, default=4)\n",
    "parser.add_argument(\"--n_modules\", type=int, default=1)\n",
    "parser.add_argument(\"--r\", type=float, default=0.5)\n",
    "\n",
    "# Encoder arguments\n",
    "parser.add_argument(\"--encoder-hidden-dim\", type=int, default=None, \n",
    "                    help=\"Hidden dimension for encoder (default: d_model - P)\")\n",
    "parser.add_argument(\"--encoder-hidden-ff\", type=int, default=128,\n",
    "                    help=\"Hidden feedforward dimension for encoder\")\n",
    "parser.add_argument(\"--patch_shape\", type=int, nargs=2, default=[4, 4],\n",
    "                    help=\"A token is a patch of size patch_shape\")\n",
    "\n",
    "# Decoder arguments\n",
    "parser.add_argument(\"--decoder-hidden-channels\", type=int, nargs=\"+\", default=[64, 256],\n",
    "                    help=\"Hidden channels for decoder MLP (excluding final output channel)\")\n",
    "\n",
    "parser.add_argument(\"--train-kind\", choices=[\"acausal\", \"causal_one_step\", \"causal_many_steps\"], default=\"acausal\",\n",
    "                    help=\"Pipeline kind to use during training\")\n",
    "parser.add_argument(\"--val-kind\", choices=[\"acausal\", \"causal_one_step\", \"causal_many_steps\"], default=\"acausal\",\n",
    "                    help=\"Pipeline kind to use during validation\")\n",
    "\n",
    "args = parser.parse_args(\"--epochs 1 --train-kind causal_one_step --val-kind causal_many_steps --no-refinement\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created run directory: runs/test/run8\n"
     ]
    }
   ],
   "source": [
    "# Create directory structure\n",
    "Path(\"runs\").mkdir(exist_ok=True)\n",
    "base_dir = Path(\"runs/\" + args.name)\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Find the next available run number\n",
    "run_num = 0\n",
    "while True:\n",
    "    run_dir = base_dir / f\"run{run_num}\"\n",
    "    if not run_dir.exists():\n",
    "        break\n",
    "    run_num += 1\n",
    "\n",
    "# Create the run directory\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "print(f\"Created run directory: {run_dir}\")\n",
    "\n",
    "# Save hyperparameters/config\n",
    "config_dict = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'lr': args.lr,\n",
    "    'weight_decay': args.weight_decay,\n",
    "    'n_timesteps': args.n_timesteps,\n",
    "    'share': args.share,\n",
    "    'refinement': args.refinement,\n",
    "    'picard': args.picard,\n",
    "    'd_model': args.d_model,\n",
    "    'nhead': args.nhead,\n",
    "    'dim_feedforward': args.dim_feedforward,\n",
    "    'dropout': args.dropout,\n",
    "    'n_layers': args.n_layers,\n",
    "    'n_modules': args.n_modules,\n",
    "    'r': args.r,\n",
    "    'encoder_hidden_dim': args.encoder_hidden_dim,\n",
    "    'encoder_hidden_ff': args.encoder_hidden_ff,\n",
    "    'patch_shape': args.patch_shape,\n",
    "    'decoder_hidden_channels': args.decoder_hidden_channels,\n",
    "    'train_kind': args.train_kind,\n",
    "    'val_kind': args.val_kind,\n",
    "}\n",
    "np.save(run_dir / \"config.npy\", config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "init_conds, trajs = load_navier_stokes_tensor(args.data, n_timesteps=args.n_timesteps)\n",
    "init_conds = init_conds.to(device)\n",
    "trajs = trajs.to(device)\n",
    "\n",
    "train_loader, val_loader = setup_dataloaders(init_conds, trajs, batch_size=args.batch_size)\n",
    "P = 3\n",
    "N, T, H, W, Q = trajs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def galerkin_init(param, gain=0.01, diagonal_weight=0.01):\n",
    "    nn.init.xavier_uniform_(param, gain=gain)\n",
    "    param.data += diagonal_weight * torch.diag(torch.ones(param.size(-1), dtype=torch.float, device=param.device))\n",
    "\n",
    "class TrimTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int, dropout: float, n_layers: int, mask: torch.Tensor | None = None, scale: float | None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        norm_k = nn.LayerNorm(d_model//nhead)\n",
    "        norm_v = nn.LayerNorm(d_model//nhead)\n",
    "        encoder_layer = TrimTransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_k=norm_k,\n",
    "            norm_v=norm_v,\n",
    "            q_weight_init=galerkin_init,\n",
    "            k_weight_init=galerkin_init,\n",
    "            v_weight_init=galerkin_init,\n",
    "            scale=scale,\n",
    "        )\n",
    "        self.mask = mask\n",
    "        self.transformer = TrimTransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, use_kv_cache: bool = False, update_kv_cache: bool = False) -> torch.Tensor:\n",
    "        if self.mask is None:\n",
    "            mask = None\n",
    "        else:\n",
    "            mask = self.mask[:x.shape[1]]\n",
    "        return self.transformer(x, mask=mask, use_kv_cache=use_kv_cache, update_kv_cache=update_kv_cache)\n",
    "\n",
    "    def clear_kv_cache(self):\n",
    "        self.transformer.clear_kv_cache()\n",
    "\n",
    "def make_block_mask_after(n_tokens, block_size):\n",
    "    idx = torch.arange(n_tokens, dtype=torch.long)\n",
    "    mask_after = block_size * ((idx // block_size) + 1)-1\n",
    "    return mask_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set encoder hidden_dim: use provided value or default to d_model - P\n",
    "encoder_hidden_dim = args.encoder_hidden_dim if args.encoder_hidden_dim is not None else args.d_model - P\n",
    "\n",
    "encoder = SingleConvNeuralNet(dim=Q,\n",
    "                                hidden_dim=encoder_hidden_dim,\n",
    "                                out_dim=args.d_model-P,\n",
    "                                hidden_ff=args.encoder_hidden_ff,\n",
    "                                K=args.patch_shape,\n",
    "                                S=args.patch_shape)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Dummy forward pass to get shapes\n",
    "with torch.no_grad():\n",
    "    _, _, H_prime, W_prime, _ = encoder.forward(trajs[0, None, ...].to(device)).shape\n",
    "block_size = H_prime * W_prime\n",
    "\n",
    "if args.refinement:\n",
    "    n_tokens = (args.n_timesteps + 1) * H_prime * W_prime\n",
    "    scale = 1 / n_tokens\n",
    "    mask = None\n",
    "    make_module = partial(TrimTransformer,\n",
    "                    d_model=args.d_model,\n",
    "                    nhead=args.nhead,\n",
    "                    dim_feedforward=args.dim_feedforward,\n",
    "                    dropout=args.dropout,\n",
    "                    n_layers=args.n_layers,\n",
    "                    mask=mask,\n",
    "                    scale=scale)\n",
    "else:\n",
    "    n_tokens = args.n_timesteps * H_prime * W_prime\n",
    "    scale = 1 / n_tokens\n",
    "    mask = make_block_mask_after(n_tokens, block_size).to(device)\n",
    "    make_module = partial(TrimTransformer,\n",
    "                          d_model=args.d_model,\n",
    "                          nhead=args.nhead,\n",
    "                          dim_feedforward=args.dim_feedforward,\n",
    "                          dropout=args.dropout,\n",
    "                          n_layers=args.n_layers,\n",
    "                          mask=mask,\n",
    "                          scale=scale)\n",
    "if args.share:\n",
    "    modules = arch.make_weight_shared_modules(make_module, n_modules=args.n_modules)\n",
    "else:\n",
    "    modules = arch.make_weight_unshared_modules(make_module, n_modules=args.n_modules)\n",
    "if args.picard:\n",
    "    model = arch.PicardIterations(modules, q=Q, r=args.r)\n",
    "else:\n",
    "    model = arch.ArbitraryIterations(modules)\n",
    "model = model.to(device)\n",
    "\n",
    "# Build decoder hidden channels: user-specified channels + fixed output channel\n",
    "decoder_hidden_channels = args.decoder_hidden_channels + [H*W*Q]\n",
    "\n",
    "decoder = MLP(\n",
    "    in_channels=H_prime*W_prime*(args.d_model-P),\n",
    "    hidden_channels=decoder_hidden_channels,\n",
    "    activation_layer=nn.ELU,\n",
    ")\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | train loss: 0.035217 | val loss: 0.323579\n",
      "\n",
      "Training completed! All files saved to: runs/test/run8\n",
      "Final train loss: 0.035217\n",
      "Final val loss: 0.323579\n"
     ]
    }
   ],
   "source": [
    "loss_fn = F.mse_loss\n",
    "optim = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(encoder.parameters()) + list(decoder.parameters()), lr=args.lr, weight_decay=args.weight_decay\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=args.epochs)\n",
    "\n",
    "train_pipeline = build_pipeline(args.train_kind,\n",
    "                                encoder=encoder,\n",
    "                                model=model,\n",
    "                                decoder=decoder)\n",
    "\n",
    "val_pipeline = build_pipeline(args.val_kind,\n",
    "                              encoder=encoder,\n",
    "                              model=model,\n",
    "                              decoder=decoder)\n",
    "\n",
    "train_trainer = Trainer(train_pipeline, loss_fn)\n",
    "\n",
    "val_trainer = Trainer(val_pipeline, loss_fn)\n",
    "\n",
    "# Initialize lists to track losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_loss = train_trainer.train_epoch(train_loader, optim)\n",
    "    with torch.no_grad():\n",
    "        val_loss   = val_trainer.eval_epoch(val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Append losses to tracking lists\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch:3d} | train loss: {train_loss:.6f} | val loss: {val_loss:.6f}\")\n",
    "    \n",
    "    # Save losses as numpy arrays every epoch in run directory\n",
    "    np.save(run_dir / \"train_loss.npy\", np.array(train_losses))\n",
    "    np.save(run_dir / \"val_loss.npy\", np.array(val_losses))\n",
    "\n",
    "# Save model weights in run directory\n",
    "torch.save({\"state_dict\": model.state_dict()}, run_dir / \"model_weights.pt\")\n",
    "\n",
    "# Save final loss arrays in run directory\n",
    "np.save(run_dir / \"train_loss.npy\", np.array(train_losses))\n",
    "np.save(run_dir / \"val_loss.npy\", np.array(val_losses))\n",
    "\n",
    "print(f\"\\nTraining completed! All files saved to: {run_dir}\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trimtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
