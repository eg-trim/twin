{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_BHPTNRSur = \"/home/ubuntu/EG-UT/BHPTNRSurrogate\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# add the path to the script directory\n",
    "sys.path.append(PATH_TO_BHPTNRSur)\n",
    "from surrogates import BHPTNRSur1dq1e4 as bhptsur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsur, hsur = bhptsur.generate_surrogate(q=2.5)\n",
    "print(hsur[(2,2)][0])\n",
    "\n",
    "#print(hsur.keys())\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(tsur, np.real(hsur[(2,2)]), '-', label='22')\n",
    "plt.plot(tsur, np.real(hsur[(3,3)]), '-', label='33')\n",
    "plt.xlabel('time [seconds]', fontsize=15)\n",
    "plt.ylabel('rh/M', fontsize=15)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot import LAL\n",
      "**** Surrogate loaded: BHPTNRSur1dq1e4 ****\n",
      "First 5 q values: [2.5 2.5 2.5 2.5 2.5]\n",
      "Generating waveforms:  32%|█████▍           | 1600/5000 [02:08<04:34, 12.40it/s]"
     ]
    }
   ],
   "source": [
    "!python3 generate_bhpt_dataset.py --n-samples 5000  --q-min 2.5 --q-max 2.5 --n-timesteps 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"python3 generate_bhpt_dataset.py --n-samples 256  --q-min 2.5 --q-max 10.0\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import architectures as arch\n",
    "from functools import partial\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from torchvision.ops import MLP\n",
    "from data import setup_waveform_dataloaders, WaveformDataset, load_bhpt_tensors\n",
    "from architectures import SingleConvNeuralNet, GalerkinTransformer\n",
    "from bhpt_running import RefinementPipeline, Trainer\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"BHPT training script (core logic excerpt).\")\n",
    "parser.add_argument(\"--data\", type=Path, default=Path(\"bhpt_dataset.pt\"), help=\"Path to the .pt dataset produced by generate_bhpt_dataset.py.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=100000)\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1000)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--weight-decay\", type=float, default=0, help=\"Weight decay (L2 penalty) for Adam optimizer.\")\n",
    "parser.add_argument(\"--n-timesteps\", type=int, default=None, help=\"Number of temporal frames to sample from the raw data (consistent with notebook).\")\n",
    "\n",
    "parser.add_argument(\"--share\", action=\"store_true\", help=\"Share weights between modules.\")\n",
    "parser.add_argument(\"--no-share\", dest=\"share\", action=\"store_false\", help=\"Don't share weights between modules.\")\n",
    "parser.set_defaults(share=True)\n",
    "\n",
    "parser.add_argument(\"--refinement\", action=\"store_true\", help=\"Use refinement.\")\n",
    "parser.add_argument(\"--no-refinement\", dest=\"refinement\", action=\"store_false\", help=\"Don't use refinement.\")\n",
    "parser.set_defaults(refinement=True)\n",
    "\n",
    "parser.add_argument(\"--picard\", action=\"store_true\", help=\"Use Picard iterations.\")\n",
    "parser.add_argument(\"--no-picard\", dest=\"picard\", action=\"store_false\", help=\"Don't use Picard iterations.\")\n",
    "parser.set_defaults(picard=True)\n",
    "\n",
    "parser.add_argument(\"--d_model\", type=int, default=31)\n",
    "parser.add_argument(\"--nhead\", type=int, default=4)\n",
    "parser.add_argument(\"--dim_feedforward\", type=int, default=64)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "parser.add_argument(\"--n_layers\", type=int, default=2)\n",
    "parser.add_argument(\"--n_modules\", type=int, default=1)\n",
    "parser.add_argument(\"--q\", type=int, default=1)\n",
    "parser.add_argument(\"--r\", type=float, default=0.5)\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path if your dataset is saved elsewhere\n",
    "dataset_path = Path(\"bhpt_dataset.pt\")\n",
    "# How many random samples to plot from the dataset\n",
    "n_to_plot = 3\n",
    "# ---------------------\n",
    "\n",
    "# 1. Load the dataset from the .pt file\n",
    "if not dataset_path.exists():\n",
    "    print(f\"Error: Dataset file not found at '{dataset_path}'\")\n",
    "    print(\"Please make sure the path is correct and you have generated the dataset.\")\n",
    "else:\n",
    "    # Plotting doesn't need a GPU, so we map to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    data = torch.load(dataset_path, map_location=device)\n",
    "    waveforms = data[\"waveforms\"]  # Shape: (N, T, 1, 1, 2)\n",
    "    params = data[\"params\"]        # Shape: (N, 1)\n",
    "\n",
    "    n_samples, n_timesteps, _, _, _ = waveforms.shape\n",
    "    print(f\"Dataset loaded with {n_samples} samples and {n_timesteps} timesteps.\")\n",
    "\n",
    "    # 2. Select random samples to plot\n",
    "    if n_samples < n_to_plot:\n",
    "        print(f\"Warning: Only {n_samples} samples available, plotting all of them.\")\n",
    "        indices_to_plot = range(n_samples)\n",
    "    else:\n",
    "        # Generate unique random indices without replacement\n",
    "        indices_to_plot = torch.randperm(n_samples)[:n_to_plot]\n",
    "\n",
    "    # 3. Create the plots\n",
    "    # Create a figure with one column and n_to_plot rows\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_to_plot,\n",
    "        ncols=1,\n",
    "        figsize=(18, 4 * n_to_plot),\n",
    "        squeeze=False  # Always return a 2D array for axes, even if nrows=1\n",
    "    )\n",
    "\n",
    "    for i, sample_idx in enumerate(indices_to_plot):\n",
    "        ax = axes[i, 0]  # Get the current subplot axis\n",
    "        \n",
    "        # Extract the data for the chosen sample\n",
    "        waveform_sample = waveforms[sample_idx]  # Shape: (T, 1, 1, 2)\n",
    "        q_value = params[sample_idx].item()      # Get scalar value\n",
    "        \n",
    "        # The time axis is just the sample index\n",
    "        time_axis = range(n_timesteps)\n",
    "        \n",
    "        # Squeeze out the singleton H and W dimensions (1, 1) to get (T, 2)\n",
    "        # then extract the plus and cross polarisations.\n",
    "        h_plus = waveform_sample.squeeze()[:, 0]\n",
    "        h_cross = waveform_sample.squeeze()[:, 1]\n",
    "        \n",
    "        # Plot h_plus and h_cross on the same axes\n",
    "        ax.plot(time_axis, h_plus, label=r'$h_+$ (plus)', lw=1.5)\n",
    "        ax.plot(time_axis, h_cross, label=r'$h_\\times$ (cross)', lw=1.5, linestyle='--')\n",
    "        \n",
    "        # --- Formatting ---\n",
    "        ax.set_title(f\"Sample #{sample_idx}: Mass Ratio q = {q_value:.3f}\")\n",
    "        ax.set_xlabel(\"Time Step Index\")\n",
    "        ax.set_ylabel(\"Strain (scaled)\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle=':', alpha=0.7)\n",
    "\n",
    "    # Add a main title to the figure and adjust layout\n",
    "    fig.suptitle(\"BHPT Dataset Waveform Visualization\", fontsize=16, y=0.99)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Load data from the .pt file into two tensors\n",
    "waveforms_tensor, params_tensor = load_bhpt_tensors(\n",
    "    args.data, n_timesteps=args.n_timesteps\n",
    ")\n",
    "\n",
    "# 2. Create DataLoaders from the tensors\n",
    "train_loader, val_loader = setup_waveform_dataloaders(\n",
    "    waveforms=waveforms_tensor.to(device),\n",
    "    params=params_tensor.to(device),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "\n",
    "# 3. Define dimensions for the model\n",
    "P = 3  # Positional encoding dimensions (t, y, x)\n",
    "N, T, H, W, Q = waveforms_tensor.shape\n",
    "_, n_params = params_tensor.shape\n",
    "\n",
    "print(\"DataLoaders are ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Definition\n",
    "# For waveform data (H=1, W=1), the \"convolution\" is just a linear projection.\n",
    "# Kernel size K and stride S are (1,1).\n",
    "\n",
    "encoder_out_dim = args.d_model - P\n",
    "\n",
    "encoder = SingleConvNeuralNet(dim=Q,\n",
    "                                hidden_dim=args.d_model-P,\n",
    "                                out_dim=args.d_model-P,\n",
    "                                hidden_ff=128,\n",
    "                                K=[1,1],\n",
    "                                S=[1,1])\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Dummy forward pass to get the shape for the decoder\n",
    "with torch.no_grad():\n",
    "    sample_waveforms = waveforms_tensor[0, None, ...].to(device)\n",
    "    _, _, H_prime, W_prime, _ = encoder.forward(sample_waveforms).shape\n",
    "# The decoder's input channels depend on the encoder's output shape\n",
    "decoder_in_channels = H_prime * W_prime * encoder_out_dim\n",
    "\n",
    "# The total dimension fed to the transformer will be d_model + n_params\n",
    "d_model_conditioned = args.d_model + n_params\n",
    "\n",
    "if args.refinement:\n",
    "    make_module = partial(arch.GalerkinTransformer,\n",
    "                          d_model=d_model_conditioned,\n",
    "                          nhead=args.nhead,\n",
    "                          dim_feedforward=args.dim_feedforward,\n",
    "                          dropout=args.dropout,\n",
    "                          n_layers=args.n_layers)\n",
    "    process_trajectory = arch.broadcast_initial_conditions\n",
    "else:\n",
    "    # Autoregressive training is not implemented for the BHPT case yet\n",
    "    raise NotImplementedError(\"Only refinement mode is set up for BHPT training.\")\n",
    "\n",
    "if args.share:\n",
    "    modules = arch.make_weight_shared_modules(make_module, n_modules=args.n_modules)\n",
    "else:\n",
    "    modules = arch.make_weight_unshared_modules(make_module, n_modules=args.n_modules)\n",
    "\n",
    "if args.picard:\n",
    "    model = arch.PicardIterations(modules, q=args.q, r=args.r)\n",
    "else:\n",
    "    model = arch.ArbitraryIterations(modules)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "decoder = MLP(\n",
    "    in_channels=decoder_in_channels,\n",
    "    hidden_channels=[64, 256, H * W * Q], # Final output must match original H, W, Q\n",
    "    activation_layer=nn.ELU,\n",
    ")\n",
    "\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "print(f\"Model, Encoder, and Decoder are on device: {next(model.parameters()).device}\")\n",
    "print(f\"Transformer d_model: {d_model_conditioned} (divisible by nhead={args.nhead})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for animation\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Animation Function ---\n",
    "# This function will be used to create a GIF of the training progress.\n",
    "def create_animation_from_data(predictions_over_time, true_wave, losses, epoch_numbers, save_path=None):\n",
    "    \"\"\"\n",
    "    Create an animation comparing model predictions to the true waveform over epochs.\n",
    "    \n",
    "    Args:\n",
    "        predictions_over_time (list): A list of numpy arrays, where each array is a\n",
    "                                      model prediction (T, 2) for a given epoch.\n",
    "        true_wave (np.array): The ground truth waveform (T, 2) to compare against.\n",
    "        losses (list): A list of tuples, each containing (train_loss, val_loss) for an epoch.\n",
    "        save_path (str or Path, optional): Path to save the animation GIF.\n",
    "    \"\"\"\n",
    "    fig_anim, ax_anim = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "    true_plus = true_wave[:, 0]\n",
    "    true_cross = true_wave[:, 1]\n",
    "    time_axis = range(len(true_plus))\n",
    "\n",
    "    # Plot the true signals as dashed lines\n",
    "    line_true_plus, = ax_anim.plot(time_axis, true_plus, 'b--', linewidth=1.5, label=r'True $h_+$', alpha=0.6)\n",
    "    line_true_cross, = ax_anim.plot(time_axis, true_cross, 'c--', linewidth=1.5, label=r'True $h_\\times$', alpha=0.6)\n",
    "    \n",
    "    # Plot the predicted signals as solid lines (data will be updated in animate)\n",
    "    line_pred_plus, = ax_anim.plot([], [], 'r-', linewidth=1.5, label=r'Pred $h_+$', alpha=0.8)\n",
    "    line_pred_cross, = ax_anim.plot([], [], 'g-', linewidth=1.5, label=r'Pred $h_\\times$', alpha=0.8)\n",
    "    \n",
    "    # Determine plot limits from the true signal for stability\n",
    "    y_min = true_wave.min() * 1.2\n",
    "    y_max = true_wave.max() * 1.2\n",
    "    ax_anim.set_xlim(0, len(true_plus))\n",
    "    ax_anim.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Formatting\n",
    "    ax_anim.set_xlabel('Time Steps')\n",
    "    ax_anim.set_ylabel('Strain (scaled)')\n",
    "    ax_anim.set_title('Training Progress: Prediction vs. Ground Truth')\n",
    "    ax_anim.legend(loc='upper right')\n",
    "    ax_anim.grid(True, linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Text box for epoch and loss information\n",
    "    text_info_anim = ax_anim.text(0.02, 0.95, '', transform=ax_anim.transAxes, fontsize=12, \n",
    "                                 bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"wheat\", alpha=0.7),\n",
    "                                 verticalalignment='top')\n",
    "    \n",
    "    # Animation function: this is called sequentially for each frame\n",
    "    def animate(frame):\n",
    "        if frame < len(predictions_over_time):\n",
    "            prediction = predictions_over_time[frame]\n",
    "            train_loss, val_loss = losses[frame]\n",
    "            \n",
    "            # Update prediction lines\n",
    "            line_pred_plus.set_data(time_axis, prediction[:, 0])\n",
    "            line_pred_cross.set_data(time_axis, prediction[:, 1])\n",
    "            \n",
    "            # Update text box\n",
    "            # Update text box\n",
    "            epoch_num = epoch_numbers[frame]\n",
    "            text_info_anim.set_text(f'Epoch: {epoch_num}\\nTrain Loss: {train_loss:.7f}\\nVal Loss: {val_loss:.7f}')\n",
    "        \n",
    "        return line_pred_plus, line_pred_cross, text_info_anim\n",
    "\n",
    "    # Create the animation\n",
    "    anim = animation.FuncAnimation(fig_anim, animate, frames=len(predictions_over_time), \n",
    "                                 interval=200, blit=True)\n",
    "    \n",
    "    # Save the animation if a path is provided\n",
    "    if save_path:\n",
    "        print(f\"Saving animation to {save_path}...\")\n",
    "        anim.save(str(save_path), writer='pillow', fps=5)\n",
    "        print(\"Save complete.\")\n",
    "    \n",
    "    plt.close(fig_anim) # Avoid displaying the static plot in the notebook\n",
    "    return anim\n",
    "\n",
    "# --- 2. Training Setup ---\n",
    "# The optimizer needs to know about all trainable parameters\n",
    "all_params = list(model.parameters()) + list(encoder.parameters()) + list(decoder.parameters())\n",
    "optim = torch.optim.Adam(all_params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=args.epochs)\n",
    "\n",
    "# Instantiate the pipeline and trainer from your notebook's logic\n",
    "pipeline = RefinementPipeline(\n",
    "    model=model,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    process_trajectory=process_trajectory\n",
    ").to(device)\n",
    "\n",
    "trainer = Trainer(pipeline)\n",
    "\n",
    "# Get a fixed sample from the validation set for consistent visualization\n",
    "try:\n",
    "    vis_wave_batch, vis_param_batch = next(iter(val_loader))\n",
    "    true_wave_for_vis = vis_wave_batch[0].squeeze().cpu().numpy()\n",
    "except StopIteration:\n",
    "    print(\"Validation loader is empty. Using a sample from the training loader for visualization.\")\n",
    "    vis_wave_batch, vis_param_batch = next(iter(train_loader))\n",
    "    true_wave_for_vis = vis_wave_batch[0].squeeze().cpu().numpy()\n",
    "\n",
    "# --- 3. Training Loop ---\n",
    "animation_save_frequency = 50\n",
    "\n",
    "predictions_history = []\n",
    "losses_history = []\n",
    "epochs_for_animation = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "save_path = Path(\"best_bhpt_weights.pt\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    # Set models to training mode\n",
    "    pipeline.train()\n",
    "    train_loss = trainer.train_epoch(train_loader, optim)\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    pipeline.eval()\n",
    "    val_loss = trainer.eval_epoch(val_loader)\n",
    "    \n",
    "    # Generate a prediction for the visualization sample\n",
    "    with torch.no_grad():\n",
    "        pred_wave = pipeline(vis_wave_batch, vis_param_batch)\n",
    "    \n",
    "    # Store history for animation\n",
    "    if epoch % animation_save_frequency == 0 or epoch == 1 or epoch == args.epochs:\n",
    "        p = pred_wave[0].squeeze().cpu().numpy().copy()\n",
    "        predictions_history.append(p)\n",
    "        losses_history.append((train_loss, val_loss))\n",
    "        epochs_for_animation.append(epoch)\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.7f} | Val Loss: {val_loss:.7f} | LR: {scheduler.get_last_lr()[0]:.1e}\")\n",
    "    if epoch % 100 == 0: \n",
    "        torch.save({\n",
    "            'epoch': epoch, \n",
    "            'model_state_dict': model.state_dict(), \n",
    "            'encoder_state_dict': encoder.state_dict(), \n",
    "            'decoder_state_dict': decoder.state_dict(), \n",
    "            'optimizer_state_dict': optim.state_dict(), 'loss': val_loss, \n",
    "            'args': args}, f\"bhpt_weights_epoch_{epoch}.pt\")\n",
    "print(f\"  -> Saving model weights to {save_path}\")\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'decoder_state_dict': decoder.state_dict(),\n",
    "    'optimizer_state_dict': optim.state_dict(),\n",
    "    'loss': val_loss,\n",
    "    'args': args\n",
    "}, save_path)\n",
    "        \n",
    "print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "# --- 4. Create and Display Animation ---\n",
    "# Generate the animation from the stored history\n",
    "# Generate the animation from the stored history\n",
    "animation_save_path = Path(\"training_progress.gif\")\n",
    "final_anim = create_animation_from_data(\n",
    "    predictions_history, \n",
    "    true_wave_for_vis, \n",
    "    losses_history, \n",
    "    epochs_for_animation, # <-- Pass the new list here\n",
    "    animation_save_path\n",
    ")\n",
    "\n",
    "# Display the animation directly in the notebook\n",
    "HTML(final_anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last dimension of the tensor holds the [real, imaginary] parts.\n",
    "# Let's calculate the complex amplitude (magnitude) for every point.\n",
    "amplitudes = torch.sqrt(waveforms_tensor[..., 0]**2 + waveforms_tensor[..., 1]**2)\n",
    "\n",
    "# Now, for each individual waveform in the dataset, find its maximum amplitude over time.\n",
    "# The time dimension is the second one (dim=1).\n",
    "peak_amplitudes_per_waveform = torch.max(amplitudes, dim=1).values\n",
    "\n",
    "# Finally, let's find the min and max of those peak amplitudes.\n",
    "# This tells us the range of the strongest signals across the whole dataset.\n",
    "smallest_peak = peak_amplitudes_per_waveform.min().item()\n",
    "largest_peak = peak_amplitudes_per_waveform.max().item()\n",
    "mean_peak = peak_amplitudes_per_waveform.mean().item()\n",
    "\n",
    "print(\"Analysis of Peak Waveform Amplitudes:\")\n",
    "print(f\"  Smallest peak amplitude in the dataset: {smallest_peak:.2e}\")\n",
    "print(f\"  Largest peak amplitude in the dataset:  {largest_peak:.2e}\")\n",
    "print(f\"  Mean peak amplitude in the dataset:     {mean_peak:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimizer needs to know about all trainable parameters\n",
    "all_params = list(model.parameters()) + list(encoder.parameters()) + list(decoder.parameters())\n",
    "optim = torch.optim.Adam(all_params, lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=args.epochs)\n",
    "\n",
    "# Instantiate the pipeline and trainer\n",
    "pipeline = RefinementPipeline(\n",
    "    model=model,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    process_trajectory=process_trajectory\n",
    ").to(device)\n",
    "\n",
    "trainer = Trainer(pipeline)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "save_path = Path(\"best_bhpt_weights.pt\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_loss = trainer.train_epoch(train_loader, optim)\n",
    "    val_loss = trainer.eval_epoch(val_loader)\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch:3d} | train loss: {train_loss:.6f} | val loss: {val_loss:.10f}\")\n",
    "\n",
    "print(\"\\nTraining complete. Saved model weights to best_bhpt_weights.pt\")\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': val_loss,\n",
    "        }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from torchvision.ops import MLP\n",
    "\n",
    "import architectures as arch\n",
    "from architectures import SingleConvNeuralNet, GalerkinTransformer\n",
    "from bhpt_running import RefinementPipeline\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0.  Surrogate model (make sure its repo is on the PYTHONPATH)\n",
    "# ----------------------------------------------------------------------\n",
    "PATH_TO_BHPTNRSur = \"/home/ubuntu/EG-UT/BHPTNRSurrogate\"\n",
    "if PATH_TO_BHPTNRSur not in sys.path:\n",
    "    sys.path.append(PATH_TO_BHPTNRSur)\n",
    "from surrogates import BHPTNRSur1dq1e4 as bhptsur\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P               = 3     # (t, y, x) positional encodings\n",
    "d_model         = 127\n",
    "nhead           = 4\n",
    "dim_ff          = 2048\n",
    "dropout         = 0.1\n",
    "n_layers        = 8\n",
    "n_modules       = 1\n",
    "q_picard        = 1\n",
    "r_picard        = 0.5\n",
    "share_weights   = True\n",
    "\n",
    "H = W = 1           # spatial size\n",
    "Q = 2               # (h₊, h×)\n",
    "\n",
    "encoder_out_dim = d_model - P\n",
    "encoder = SingleConvNeuralNet(\n",
    "    dim=Q,\n",
    "    hidden_dim=encoder_out_dim,\n",
    "    out_dim=encoder_out_dim,\n",
    "    hidden_ff=128,\n",
    "    K=[1, 1],\n",
    "    S=[1, 1],\n",
    ").to(device)\n",
    "\n",
    "make_module = partial(\n",
    "    arch.GalerkinTransformer,\n",
    "    d_model=d_model + 1,      # +1 for the scalar mass-ratio parameter\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_ff,\n",
    "    dropout=dropout,\n",
    "    n_layers=n_layers,\n",
    ")\n",
    "modules = (\n",
    "    arch.make_weight_shared_modules(make_module, n_modules)\n",
    "    if share_weights else\n",
    "    arch.make_weight_unshared_modules(make_module, n_modules)\n",
    ")\n",
    "model = arch.PicardIterations(modules, q=q_picard, r=r_picard).to(device)\n",
    "\n",
    "decoder_in = encoder_out_dim * H * W\n",
    "decoder = MLP(\n",
    "    in_channels=decoder_in,\n",
    "    hidden_channels=[64, 256, H * W * Q],\n",
    "    activation_layer=torch.nn.ELU,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"bhpt_weights_epoch_96600.pt\", map_location=device, weights_only=False)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "encoder.load_state_dict(ckpt[\"encoder_state_dict\"])\n",
    "decoder.load_state_dict(ckpt[\"decoder_state_dict\"])\n",
    "\n",
    "model.eval(); encoder.eval(); decoder.eval()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Build a test input from the surrogate (use ANY q in [2.5, 1e4])\n",
    "# ----------------------------------------------------------------------\n",
    "modes_used = (\n",
    "    (2, 2),\n",
    ")\n",
    "\n",
    "q_test = 2.5\n",
    "tsur, hdict = bhptsur.generate_surrogate(q=q_test)\n",
    "\n",
    "# Sum the requested modes, then split into plus / cross\n",
    "h_sum = sum(hdict[m] for m in modes_used)          # complex array\n",
    "h_plus, h_cross = np.real(h_sum) * 20 , np.imag(h_sum) * 20  # (T,)\n",
    "\n",
    "# Slice to first 500 time steps\n",
    "n_timesteps = len(tsur)\n",
    "tsur = tsur[:n_timesteps]\n",
    "h_plus = h_plus[:n_timesteps]\n",
    "h_cross = h_cross[:n_timesteps]\n",
    "\n",
    "wave_np = np.stack([h_plus, h_cross], axis=-1).astype(np.float32)  # (T, 2)\n",
    "wave_np = wave_np[:, None, None, :]                                # (T, 1, 1, 2)\n",
    "wave_t  = torch.from_numpy(wave_np)[None].to(device)               # (1, T, 1, 1, 2)\n",
    "param_t = torch.tensor([[q_test]], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the pipeline for inference\n",
    "inference_pipeline = RefinementPipeline(\n",
    "    model=model,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    process_trajectory=arch.broadcast_initial_conditions,\n",
    ").to(device)\n",
    "inference_pipeline.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = inference_pipeline(wave_t, param_t)\n",
    "\n",
    "pred_np   = pred.squeeze(0).cpu().numpy()[:, 0, 0, :]   # (T, 2)\n",
    "pred_plus = pred_np[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(tsur, h_plus,  label=\"Surrogate ∑ modes\",      lw=1.0)\n",
    "plt.plot(tsur, pred_plus, label=\"Transformer output\",   lw=1.0)\n",
    "plt.xlabel(\"time [seconds]\", fontsize=14)\n",
    "plt.ylabel(\"rh/M\",          fontsize=14)\n",
    "plt.title(f\"Mass-ratio q = {q_test}\", fontsize=15)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
